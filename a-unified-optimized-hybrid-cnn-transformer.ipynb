{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2169393,"sourceType":"datasetVersion","datasetId":1302315}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nprint(os.listdir(\"/kaggle/input\"))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:32.062222Z","iopub.execute_input":"2025-12-25T08:44:32.062786Z","iopub.status.idle":"2025-12-25T08:44:32.066648Z","shell.execute_reply.started":"2025-12-25T08:44:32.062761Z","shell.execute_reply":"2025-12-25T08:44:32.066114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nprint(os.listdir(\"/kaggle/input/chexpert\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:32.068474Z","iopub.execute_input":"2025-12-25T08:44:32.068946Z","iopub.status.idle":"2025-12-25T08:44:32.101216Z","shell.execute_reply.started":"2025-12-25T08:44:32.068925Z","shell.execute_reply":"2025-12-25T08:44:32.100637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BASE_PATH = \"/kaggle/input/chexpert\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:32.102368Z","iopub.execute_input":"2025-12-25T08:44:32.102559Z","iopub.status.idle":"2025-12-25T08:44:32.114725Z","shell.execute_reply.started":"2025-12-25T08:44:32.102536Z","shell.execute_reply":"2025-12-25T08:44:32.114131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.listdir(BASE_PATH))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:32.115422Z","iopub.execute_input":"2025-12-25T08:44:32.115591Z","iopub.status.idle":"2025-12-25T08:44:32.129270Z","shell.execute_reply.started":"2025-12-25T08:44:32.115575Z","shell.execute_reply":"2025-12-25T08:44:32.128582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# âœ… STAGE 1 â€” PREPROCESSING (FULL DATA, CONFIRMED)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nBASE_PATH = \"/kaggle/input/chexpert\"\n\nLABELS = [\n    \"No Finding\",\n    \"Pneumonia\",\n    \"Edema\",\n    \"Atelectasis\",\n    \"Cardiomegaly\"\n]\n\ndef preprocess(csv_path, out_path):\n    df = pd.read_csv(csv_path)\n\n    # Only frontal images\n    df = df[df[\"Frontal/Lateral\"] == \"Frontal\"]\n\n    # Keep only required columns\n    df = df[[\"Path\"] + LABELS]\n\n    # Handle uncertainty labels (-1 â†’ 0)\n    df[LABELS] = df[LABELS].replace(-1, 0)\n\n    df.to_csv(out_path, index=False)\n    print(f\"Saved {len(df)} samples â†’ {out_path}\")\n\npreprocess(\n    f\"{BASE_PATH}/train.csv\",\n    \"/kaggle/working/train_full.csv\"\n)\n\npreprocess(\n    f\"{BASE_PATH}/valid.csv\",\n    \"/kaggle/working/valid_full.csv\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:32.130694Z","iopub.execute_input":"2025-12-25T08:44:32.130906Z","iopub.status.idle":"2025-12-25T08:44:33.252677Z","shell.execute_reply.started":"2025-12-25T08:44:32.130878Z","shell.execute_reply":"2025-12-25T08:44:33.251979Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# âœ… STAGE 2 â€” MEDICAL-GRADE IMAGE PREPROCESSING (CRITICAL)\n\nðŸ”¹ Radiology-optimized transforms (CLAHE + safe augmentation)","metadata":{}},{"cell_type":"code","source":"import torchvision.transforms as T\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nclass CLAHE:\n    def __call__(self, img):\n        img = np.array(img)\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n        img = clahe.apply(img)\n        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n        return Image.fromarray(img)\n\ndef get_train_transforms():\n    return T.Compose([\n        T.Resize((224, 224)),\n        CLAHE(),\n        T.RandomHorizontalFlip(p=0.5),\n        T.RandomRotation(10),\n        T.ToTensor(),\n        T.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    ])\n\ndef get_val_transforms():\n    return T.Compose([\n        T.Resize((224, 224)),\n        CLAHE(),\n        T.ToTensor(),\n        T.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:33.253455Z","iopub.execute_input":"2025-12-25T08:44:33.253661Z","iopub.status.idle":"2025-12-25T08:44:33.259935Z","shell.execute_reply.started":"2025-12-25T08:44:33.253642Z","shell.execute_reply":"2025-12-25T08:44:33.259212Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# âœ… STAGE 3 â€” KAGGLE-READY DATALOADER (FULL DATA)","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass CheXpertDataset(Dataset):\n    def __init__(self, csv_file, image_root, transform=None):\n        self.df = pd.read_csv(csv_file)\n        self.image_root = image_root\n        self.transform = transform\n\n        self.labels = [\n            \"No Finding\",\n            \"Pneumonia\",\n            \"Edema\",\n            \"Atelectasis\",\n            \"Cardiomegaly\"\n        ]\n\n        # ðŸ”‘ Force labels to float (robust fix)\n        self.df[self.labels] = (\n            self.df[self.labels]\n            .apply(pd.to_numeric, errors=\"coerce\")\n            .fillna(0.0)\n            .astype(\"float32\")\n        )\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        img_path = os.path.join(self.image_root, row[\"Path\"])\n        image = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform:\n            image = self.transform(image)\n\n        label = torch.from_numpy(\n            row[self.labels].values.astype(\"float32\")\n        )\n\n        return image, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:33.260775Z","iopub.execute_input":"2025-12-25T08:44:33.261054Z","iopub.status.idle":"2025-12-25T08:44:33.276896Z","shell.execute_reply.started":"2025-12-25T08:44:33.261034Z","shell.execute_reply":"2025-12-25T08:44:33.276303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nprint(\"Files in /kaggle/working:\")\nprint(os.listdir(\"/kaggle/working\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:33.278831Z","iopub.execute_input":"2025-12-25T08:44:33.279266Z","iopub.status.idle":"2025-12-25T08:44:33.293466Z","shell.execute_reply.started":"2025-12-25T08:44:33.279245Z","shell.execute_reply":"2025-12-25T08:44:33.292906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndef fix_paths(csv_in, csv_out):\n    df = pd.read_csv(csv_in)\n\n    # Remove CheXpert-v1.0-small prefix if present\n    df[\"Path\"] = df[\"Path\"].str.replace(\n        \"CheXpert-v1.0-small/\", \"\", regex=False\n    )\n\n    df.to_csv(csv_out, index=False)\n    print(f\"âœ… Created: {csv_out}\")\n\nfix_paths(\n    \"/kaggle/working/train_full.csv\",\n    \"/kaggle/working/train_full_fixed.csv\"\n)\n\nfix_paths(\n    \"/kaggle/working/valid_full.csv\",\n    \"/kaggle/working/valid_full_fixed.csv\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:33.294289Z","iopub.execute_input":"2025-12-25T08:44:33.294596Z","iopub.status.idle":"2025-12-25T08:44:34.153375Z","shell.execute_reply.started":"2025-12-25T08:44:33.294576Z","shell.execute_reply":"2025-12-25T08:44:34.152646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = CheXpertDataset(\n    \"/kaggle/working/train_full_fixed.csv\",\n    \"/kaggle/input/chexpert\",\n    transform=get_train_transforms()\n)\n\nval_dataset = CheXpertDataset(\n    \"/kaggle/working/valid_full_fixed.csv\",\n    \"/kaggle/input/chexpert\",\n    transform=get_val_transforms()\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:34.154310Z","iopub.execute_input":"2025-12-25T08:44:34.154601Z","iopub.status.idle":"2025-12-25T08:44:34.335328Z","shell.execute_reply.started":"2025-12-25T08:44:34.154578Z","shell.execute_reply":"2025-12-25T08:44:34.334498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    shuffle=True,\n    num_workers=0\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=0\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:34.336329Z","iopub.execute_input":"2025-12-25T08:44:34.336707Z","iopub.status.idle":"2025-12-25T08:44:34.344599Z","shell.execute_reply.started":"2025-12-25T08:44:34.336678Z","shell.execute_reply":"2025-12-25T08:44:34.344130Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ”¥ STAGE 4 â€” CLASS IMBALANCE + LOSS ENGINEERING (CRITICAL)\n\nYour earlier results (F1 â‰ˆ 0.38â€“0.43) are textbook symptoms of class imbalance, not model weakness.\n\nCheXpert facts:\n\nNo Finding dominates\n\nPneumonia, Edema, etc. are rare\n\nBCE loss treats all classes equally â†’ model collapses to majority bias\n\nSo we fix this properly.","metadata":{}},{"cell_type":"markdown","source":"âœ… 4.1 Analyze Class Distribution (MANDATORY)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"/kaggle/working/train_full.csv\")\n\nLABELS = [\n    \"No Finding\",\n    \"Pneumonia\",\n    \"Edema\",\n    \"Atelectasis\",\n    \"Cardiomegaly\"\n]\n\npos_counts = df[LABELS].sum()\nneg_counts = len(df) - pos_counts\n\nclass_stats = pd.DataFrame({\n    \"Positive\": pos_counts,\n    \"Negative\": neg_counts,\n    \"Pos_Ratio\": pos_counts / len(df)\n})\n\nclass_stats\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:34.345705Z","iopub.execute_input":"2025-12-25T08:44:34.346156Z","iopub.status.idle":"2025-12-25T08:44:34.596897Z","shell.execute_reply.started":"2025-12-25T08:44:34.346127Z","shell.execute_reply":"2025-12-25T08:44:34.596293Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"âœ… 4.2 Compute Class Weights (FOR LOSS)","metadata":{}},{"cell_type":"code","source":"import torch\n\npos_weight = neg_counts / (pos_counts + 1e-6)\npos_weight = torch.tensor(pos_weight.values, dtype=torch.float32)\n\npos_weight\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:34.597722Z","iopub.execute_input":"2025-12-25T08:44:34.597947Z","iopub.status.idle":"2025-12-25T08:44:34.619520Z","shell.execute_reply.started":"2025-12-25T08:44:34.597927Z","shell.execute_reply":"2025-12-25T08:44:34.618865Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"âœ… 4.3 Use Focal Loss + Pos Weights (KEY UPGRADE)\n\nðŸ”¹ Define Focal Loss (multi-label)","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1.0, gamma=2.0, pos_weight=None):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.pos_weight = pos_weight\n\n    def forward(self, logits, targets):\n        bce = F.binary_cross_entropy_with_logits(\n            logits,\n            targets,\n            reduction=\"none\",\n            pos_weight=self.pos_weight\n        )\n\n        probs = torch.sigmoid(logits)\n        pt = torch.where(targets == 1, probs, 1 - probs)\n\n        focal = self.alpha * (1 - pt) ** self.gamma * bce\n        return focal.mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:34.620418Z","iopub.execute_input":"2025-12-25T08:44:34.620999Z","iopub.status.idle":"2025-12-25T08:44:34.625605Z","shell.execute_reply.started":"2025-12-25T08:44:34.620978Z","shell.execute_reply":"2025-12-25T08:44:34.625093Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"âœ… 4.4 Hybrid CNNâ€“Transformer v2 (FINAL, STABLE)\n\nDo not change architecture yet â€” fix learning first.\n\nðŸ”¹ Model stays as-is\nðŸ”¹ Training changes","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:34.626433Z","iopub.execute_input":"2025-12-25T08:44:34.626830Z","iopub.status.idle":"2025-12-25T08:44:34.638946Z","shell.execute_reply.started":"2025-12-25T08:44:34.626809Z","shell.execute_reply":"2025-12-25T08:44:34.638393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class HybridCNNTransformer(nn.Module):\n    def __init__(self, num_classes=5, embed_dim=256, dropout=0.3):\n        super().__init__()\n\n        # CNN backbone (ResNet18)\n        backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        self.cnn = nn.Sequential(*list(backbone.children())[:-2])  # remove FC\n\n        self.conv_proj = nn.Conv2d(512, embed_dim, kernel_size=1)\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=4,\n            dim_feedforward=embed_dim * 4,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.classifier = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        x = self.cnn(x)                    # [B, 512, H, W]\n        x = self.conv_proj(x)              # [B, D, H, W]\n\n        B, D, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)   # [B, HW, D]\n\n        x = self.transformer(x)            # [B, HW, D]\n\n        x = x.transpose(1, 2)              # [B, D, HW]\n        x = self.pool(x).squeeze(-1)       # [B, D]\n\n        return self.classifier(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:34.641087Z","iopub.execute_input":"2025-12-25T08:44:34.641298Z","iopub.status.idle":"2025-12-25T08:44:34.653444Z","shell.execute_reply.started":"2025-12-25T08:44:34.641280Z","shell.execute_reply":"2025-12-25T08:44:34.652908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.optim import AdamW\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nmodel = HybridCNNTransformer(\n    num_classes=5,\n    embed_dim=256,\n    dropout=0.3\n).to(device)\n\ncriterion = FocalLoss(\n    alpha=1.0,\n    gamma=2.0,\n    pos_weight=pos_weight.to(device)\n)\n\noptimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:34.654151Z","iopub.execute_input":"2025-12-25T08:44:34.654595Z","iopub.status.idle":"2025-12-25T08:44:35.375710Z","shell.execute_reply.started":"2025-12-25T08:44:34.654575Z","shell.execute_reply":"2025-12-25T08:44:35.375168Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸš€ STAGE 4.5 â€” TRAIN WITH FOCAL LOSS (KAGGLE, FULL DATA)","metadata":{}},{"cell_type":"markdown","source":"âœ… 4.5.1 Training Loop (FINAL, STABLE)","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train_epoch(model, loader, optimizer, criterion):\n    model.train()\n    total_loss = 0\n\n    for x, y in tqdm(loader):\n        x = x.to(device)\n        y = y.to(device)\n\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = criterion(logits, y)\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:35.376559Z","iopub.execute_input":"2025-12-25T08:44:35.376800Z","iopub.status.idle":"2025-12-25T08:44:35.381413Z","shell.execute_reply.started":"2025-12-25T08:44:35.376772Z","shell.execute_reply":"2025-12-25T08:44:35.380806Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"âœ… 4.5.2 Train the Model (IMPORTANT SETTINGS)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef fix_paths(csv_in, csv_out):\n    df = pd.read_csv(csv_in)\n\n    # Remove the extra prefix if present\n    df[\"Path\"] = df[\"Path\"].str.replace(\n        \"CheXpert-v1.0-small/\", \"\", regex=False\n    )\n\n    df.to_csv(csv_out, index=False)\n    print(f\"Fixed paths â†’ {csv_out}\")\n\nfix_paths(\n    \"/kaggle/working/train_full.csv\",\n    \"/kaggle/working/train_full_fixed.csv\"\n)\n\nfix_paths(\n    \"/kaggle/working/valid_full.csv\",\n    \"/kaggle/working/valid_full_fixed.csv\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:35.382159Z","iopub.execute_input":"2025-12-25T08:44:35.382464Z","iopub.status.idle":"2025-12-25T08:44:36.292652Z","shell.execute_reply.started":"2025-12-25T08:44:35.382435Z","shell.execute_reply":"2025-12-25T08:44:36.291938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = CheXpertDataset(\n    \"/kaggle/working/train_full_fixed.csv\",\n    \"/kaggle/input/chexpert\",\n    transform=get_train_transforms()\n)\n\nval_dataset = CheXpertDataset(\n    \"/kaggle/working/valid_full_fixed.csv\",\n    \"/kaggle/input/chexpert\",\n    transform=get_val_transforms()\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:36.293568Z","iopub.execute_input":"2025-12-25T08:44:36.293798Z","iopub.status.idle":"2025-12-25T08:44:36.481670Z","shell.execute_reply.started":"2025-12-25T08:44:36.293778Z","shell.execute_reply":"2025-12-25T08:44:36.481150Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ðŸ”µ STEP 1 â€” Recreate DataLoaders (MANDATORY)","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:36.482535Z","iopub.execute_input":"2025-12-25T08:44:36.483116Z","iopub.status.idle":"2025-12-25T08:44:36.490672Z","shell.execute_reply.started":"2025-12-25T08:44:36.483082Z","shell.execute_reply":"2025-12-25T08:44:36.489932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ðŸ”µ STEP 2 â€” Define Optimizer + Loss (CONFIRM)","metadata":{}},{"cell_type":"code","source":"from torch.optim import AdamW\n\ncriterion = FocalLoss(\n    alpha=1.0,\n    gamma=2.0,\n    pos_weight=pos_weight.to(device)\n)\n\noptimizer = AdamW(\n    model.parameters(),\n    lr=1e-4,\n    weight_decay=1e-4\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:36.491631Z","iopub.execute_input":"2025-12-25T08:44:36.491863Z","iopub.status.idle":"2025-12-25T08:44:36.504925Z","shell.execute_reply.started":"2025-12-25T08:44:36.491823Z","shell.execute_reply":"2025-12-25T08:44:36.504306Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ðŸ”µ STEP 3 â€” START TRAINING (THIS IS THE MAIN RUN)","metadata":{}},{"cell_type":"code","source":"EPOCHS = 8\n\nfor epoch in range(EPOCHS):\n    loss = train_epoch(model, train_loader, optimizer, criterion)\n    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Loss: {loss:.4f}\")\n\ntorch.save(model.state_dict(), \"/kaggle/working/hybrid_uo_hct.pt\")\nprint(\"âœ… Model saved\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T08:44:36.505619Z","iopub.execute_input":"2025-12-25T08:44:36.506050Z","iopub.status.idle":"2025-12-25T11:04:42.111887Z","shell.execute_reply.started":"2025-12-25T08:44:36.506020Z","shell.execute_reply":"2025-12-25T11:04:42.111048Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EVALUATE (FINAL METRICS)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nmodel.load_state_dict(torch.load(\"/kaggle/working/hybrid_uo_hct.pt\"))\nmodel.eval()\n\nall_preds, all_labels = [], []\n\nwith torch.no_grad():\n    for x, y in val_loader:\n        x = x.to(device)\n        logits = model(x)\n        probs = torch.sigmoid(logits).cpu().numpy()\n\n        all_preds.append(probs)\n        all_labels.append(y.numpy())\n\ny_pred = np.vstack(all_preds)\ny_true = np.vstack(all_labels)\n\nprint(\"\\nHYBRID UO-HCT PERFORMANCE\")\nprint(f\"Accuracy : {accuracy_score(y_true > 0.5, y_pred > 0.5):.4f}\")\nprint(f\"Precision: {precision_score(y_true, y_pred > 0.5, average='macro', zero_division=0):.4f}\")\nprint(f\"Recall   : {recall_score(y_true, y_pred > 0.5, average='macro', zero_division=0):.4f}\")\nprint(f\"F1-score : {f1_score(y_true, y_pred > 0.5, average='macro', zero_division=0):.4f}\")\nprint(f\"AUC      : {roc_auc_score(y_true, y_pred, average='macro'):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T11:14:46.428875Z","iopub.execute_input":"2025-12-25T11:14:46.429703Z","iopub.status.idle":"2025-12-25T11:14:51.580970Z","shell.execute_reply.started":"2025-12-25T11:14:46.429667Z","shell.execute_reply":"2025-12-25T11:14:51.580139Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1ï¸âƒ£ CNN BASELINE â€” TRAIN & SAVE","metadata":{}},{"cell_type":"code","source":"model = CNNBaseline(num_classes=5).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = torch.nn.BCEWithLogitsLoss()\n\nEPOCHS = 5  # 5 is enough for tables\n\nfor epoch in range(EPOCHS):\n    loss = train_epoch(model, train_loader, optimizer, criterion)\n    print(f\"[CNN] Epoch {epoch+1}/{EPOCHS} - Loss: {loss:.4f}\")\n\ntorch.save(model.state_dict(), \"/kaggle/working/cnn_baseline.pt\")\nprint(\"âœ… Saved cnn_baseline.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T11:25:41.592494Z","iopub.execute_input":"2025-12-25T11:25:41.593032Z","iopub.status.idle":"2025-12-25T13:54:12.943125Z","shell.execute_reply.started":"2025-12-25T11:25:41.593005Z","shell.execute_reply":"2025-12-25T13:54:12.942278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2ï¸âƒ£ CNN + TRANSFORMER (NO OPTIMIZATION)","metadata":{}},{"cell_type":"code","source":"model = HybridCNNTransformer(num_classes=5).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = torch.nn.BCEWithLogitsLoss()\n\nfor epoch in range(EPOCHS):\n    loss = train_epoch(model, train_loader, optimizer, criterion)\n    print(f\"[CNN+T] Epoch {epoch+1}/{EPOCHS} - Loss: {loss:.4f}\")\n\ntorch.save(model.state_dict(), \"/kaggle/working/cnn_transformer_no_opt.pt\")\nprint(\"âœ… Saved cnn_transformer_no_opt.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T14:12:02.920512Z","iopub.execute_input":"2025-12-25T14:12:02.920819Z","iopub.status.idle":"2025-12-25T16:40:21.267723Z","shell.execute_reply.started":"2025-12-25T14:12:02.920793Z","shell.execute_reply":"2025-12-25T16:40:21.266974Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3ï¸âƒ£ FULL UO-HCT (FOCAL LOSS / OPTIMIZED)\n\nUse your best-performing setup (you already got AUC â‰ˆ 0.85, excellent).","metadata":{}},{"cell_type":"code","source":"# model = HybridCNNTransformer(num_classes=5).to(device)\n\n# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n# criterion = FocalLoss(alpha=0.25, gamma=2.0)\n\n# for epoch in range(EPOCHS):\n#     loss = train_epoch(model, train_loader, optimizer, criterion)\n#     print(f\"[UO-HCT] Epoch {epoch+1}/{EPOCHS} - Loss: {loss:.4f}\")\n\n# torch.save(model.state_dict(), \"/kaggle/working/hybrid_uo_hct_2.pt\")\n# print(\"âœ… Saved hybrid_uo_hct_2.pt\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DEFINE ALL MODELS EXPLICITLY (RECOMMENDED)\n\nThis is the safest, reviewer-proof approach: define both models directly in the notebook.","metadata":{}},{"cell_type":"markdown","source":"ðŸ”µ STEP 1 â€” DEFINE CNN BASELINE","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass CNNBaseline(nn.Module):\n    def __init__(self, num_classes=5):\n        super().__init__()\n        backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        self.features = nn.Sequential(*list(backbone.children())[:-1])\n        self.classifier = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        return self.classifier(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T16:40:56.593716Z","iopub.execute_input":"2025-12-25T16:40:56.594425Z","iopub.status.idle":"2025-12-25T16:40:56.599239Z","shell.execute_reply.started":"2025-12-25T16:40:56.594395Z","shell.execute_reply":"2025-12-25T16:40:56.598654Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ðŸ”µ STEP 2 â€” DEFINE HYBRID CNNâ€“TRANSFORMER","metadata":{}},{"cell_type":"code","source":"class HybridCNNTransformer(nn.Module):\n    def __init__(self, num_classes=5, embed_dim=256, dropout=0.3):\n        super().__init__()\n\n        backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        self.cnn = nn.Sequential(*list(backbone.children())[:-2])\n\n        self.conv_proj = nn.Conv2d(512, embed_dim, kernel_size=1)\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=4,\n            dim_feedforward=embed_dim * 4,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.classifier = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = self.conv_proj(x)\n\n        B, D, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n\n        x = self.transformer(x)\n        x = x.transpose(1, 2)\n        x = self.pool(x).squeeze(-1)\n\n        return self.classifier(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T16:40:57.398778Z","iopub.execute_input":"2025-12-25T16:40:57.399454Z","iopub.status.idle":"2025-12-25T16:40:57.407763Z","shell.execute_reply.started":"2025-12-25T16:40:57.399416Z","shell.execute_reply":"2025-12-25T16:40:57.406797Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ðŸ”µ STEP 0 â€” ASSUMPTIONS (VERIFY)\n\nYou should already have (or will train):\n\ncnn_baseline.pt\ncnn_transformer_no_opt.pt\nhybrid_uo_hct.pt        (FULL model)\n\n\nAll trained on full CheXpert.","metadata":{}},{"cell_type":"markdown","source":"ðŸ”µ STEP 1 â€” COMMON EVALUATION FUNCTION (RUN FIRST)","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import f1_score, roc_auc_score\nfrom torch.utils.data import DataLoader\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef evaluate_model(model, loader):\n    model.eval()\n    all_preds, all_labels = [], []\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            logits = model(x)\n            probs = torch.sigmoid(logits).cpu().numpy()\n\n            all_preds.append(probs)\n            all_labels.append(y.numpy())\n\n    y_pred = np.vstack(all_preds)\n    y_true = np.vstack(all_labels)\n\n    f1 = f1_score(y_true, y_pred > 0.5, average=\"macro\", zero_division=0)\n    auc = roc_auc_score(y_true, y_pred, average=\"macro\")\n\n    return f1, auc, y_true, y_pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T16:41:00.140557Z","iopub.execute_input":"2025-12-25T16:41:00.141233Z","iopub.status.idle":"2025-12-25T16:41:00.146757Z","shell.execute_reply.started":"2025-12-25T16:41:00.141205Z","shell.execute_reply":"2025-12-25T16:41:00.146080Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ðŸ”µ STEP 2 â€” TABLE I: OVERALL COMPARISON","metadata":{}},{"cell_type":"code","source":"models_info = {\n    \"CNN Baseline\": (\"cnn_baseline.pt\", CNNBaseline),\n    \"CNN + Transformer (No Optimization)\": (\"cnn_transformer_no_opt.pt\", HybridCNNTransformer),\n    \"Proposed UO-HCT\": (\"hybrid_uo_hct.pt\", HybridCNNTransformer),\n}\n\noverall_results = []\n\nfor name, (ckpt, model_cls) in models_info.items():\n    model = model_cls(num_classes=5).to(device)\n    model.load_state_dict(torch.load(f\"/kaggle/working/{ckpt}\", map_location=device))\n\n    f1, auc, _, _ = evaluate_model(model, val_loader)\n\n    overall_results.append({\n        \"Model\": name,\n        \"F1-score\": round(f1, 4),\n        \"AUC\": round(auc, 4)\n    })\n\noverall_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T16:41:02.115464Z","iopub.execute_input":"2025-12-25T16:41:02.115740Z","iopub.status.idle":"2025-12-25T16:41:06.956942Z","shell.execute_reply.started":"2025-12-25T16:41:02.115715Z","shell.execute_reply":"2025-12-25T16:41:06.956310Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ðŸ”µ STEP 3 â€” TABLE II: PER-CLASS PERFORMANCE (UO-HCT ONLY)","metadata":{}},{"cell_type":"code","source":"LABELS = [\n    \"No Finding\",\n    \"Pneumonia\",\n    \"Edema\",\n    \"Atelectasis\",\n    \"Cardiomegaly\"\n]\n\nmodel = HybridCNNTransformer(num_classes=5).to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/working/hybrid_uo_hct.pt\", map_location=device))\n\n_, _, y_true, y_pred = evaluate_model(model, val_loader)\n\nper_class_results = []\n\nfor i, label in enumerate(LABELS):\n    f1 = f1_score(y_true[:, i], y_pred[:, i] > 0.5, zero_division=0)\n    auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n\n    per_class_results.append({\n        \"Disease\": label,\n        \"F1-score\": round(f1, 4),\n        \"AUC\": round(auc, 4)\n    })\n\nper_class_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T16:41:06.958003Z","iopub.execute_input":"2025-12-25T16:41:06.958198Z","iopub.status.idle":"2025-12-25T16:41:08.470808Z","shell.execute_reply.started":"2025-12-25T16:41:06.958179Z","shell.execute_reply":"2025-12-25T16:41:08.470221Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":".\n\nðŸ”µ STEP 4 â€” TABLE III: ABLATION STUDY\n\nTrain or load these variants:\n\nVariant\tDescription\nFull UO-HCT\tCNN + Transformer + Focal\nWithout Transformer\tCNN only head\nWithout Optimization\tCNN + Transformer + BCE\nCNN Only\tBaseline","metadata":{}},{"cell_type":"code","source":"import os\n\nprint(os.listdir(\"/kaggle/working\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:01:40.061351Z","iopub.execute_input":"2025-12-25T17:01:40.061946Z","iopub.status.idle":"2025-12-25T17:01:40.066014Z","shell.execute_reply.started":"2025-12-25T17:01:40.061919Z","shell.execute_reply":"2025-12-25T17:01:40.065294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\n\nablation_models = {\n    \"Full UO-HCT\": (\"hybrid_uo_hct.pt\", HybridCNNTransformer),\n    \"Without Transformer\": (\"cnn_baseline.pt\", CNNBaseline),\n    \"Without Optimization\": (\"cnn_transformer_no_opt.pt\", HybridCNNTransformer),\n}\n\nablation_results = []\n\nfor name, (ckpt, model_cls) in ablation_models.items():\n    ckpt_path = f\"/kaggle/working/{ckpt}\"\n\n    print(f\"Loading {name} from {ckpt_path}\")\n\n    model = model_cls(num_classes=5).to(device)\n    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n    model.eval()\n\n    f1, auc, _, _ = evaluate_model(model, val_loader)\n\n    ablation_results.append({\n        \"Configuration\": name,\n        \"F1-score\": round(f1, 4),\n        \"AUC\": round(auc, 4)\n    })\n\nablation_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:02:33.184873Z","iopub.execute_input":"2025-12-25T17:02:33.185598Z","iopub.status.idle":"2025-12-25T17:02:37.993904Z","shell.execute_reply.started":"2025-12-25T17:02:33.185570Z","shell.execute_reply":"2025-12-25T17:02:37.993290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nmodels = [\"CNN\", \"CNN+Transformer\", \"UO-HCT\"]\nf1 = [0.1893, 0.2771, 0.5180]\nauc = [0.8220, 0.8146, 0.8509]\n\nx = range(len(models))\n\nplt.figure(figsize=(6,4))\nplt.bar(x, f1, width=0.4, label=\"F1-score\")\nplt.bar([i+0.4 for i in x], auc, width=0.4, label=\"AUC\")\nplt.xticks([i+0.2 for i in x], models)\nplt.ylabel(\"Score\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"performance_comparison.png\", dpi=300)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:13:31.158550Z","iopub.execute_input":"2025-12-25T17:13:31.159052Z","iopub.status.idle":"2025-12-25T17:13:31.455780Z","shell.execute_reply.started":"2025-12-25T17:13:31.159024Z","shell.execute_reply":"2025-12-25T17:13:31.455160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"configs = [\"Full UO-HCT\", \"Without Transformer\", \"Without Optimization\"]\nf1_ablation = [0.5180, 0.1893, 0.2771]\n\nplt.figure(figsize=(5,4))\nplt.bar(configs, f1_ablation)\nplt.ylabel(\"F1-score\")\nplt.xticks(rotation=15)\nplt.tight_layout()\nplt.savefig(\"ablation_f1.png\", dpi=300)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:13:47.039597Z","iopub.execute_input":"2025-12-25T17:13:47.039913Z","iopub.status.idle":"2025-12-25T17:13:47.250972Z","shell.execute_reply.started":"2025-12-25T17:13:47.039886Z","shell.execute_reply":"2025-12-25T17:13:47.250271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:14:57.228145Z","iopub.execute_input":"2025-12-25T17:14:57.228637Z","iopub.status.idle":"2025-12-25T17:14:57.232423Z","shell.execute_reply.started":"2025-12-25T17:14:57.228609Z","shell.execute_reply":"2025-12-25T17:14:57.231550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GradCAM:\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n        self._register_hooks()\n\n    def _register_hooks(self):\n        def forward_hook(module, input, output):\n            self.activations = output.detach()\n\n        def backward_hook(module, grad_input, grad_output):\n            self.gradients = grad_output[0].detach()\n\n        self.target_layer.register_forward_hook(forward_hook)\n        self.target_layer.register_backward_hook(backward_hook)\n\n    def generate(self, input_tensor, class_idx):\n        self.model.zero_grad()\n        output = self.model(input_tensor)\n\n        score = output[:, class_idx]\n        score.backward()\n\n        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n        cam = torch.sum(weights * self.activations, dim=1)\n\n        cam = F.relu(cam)\n        cam = cam.squeeze().cpu().numpy()\n        cam = (cam - cam.min()) / (cam.max() + 1e-8)\n\n        return cam\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:15:05.531437Z","iopub.execute_input":"2025-12-25T17:15:05.531981Z","iopub.status.idle":"2025-12-25T17:15:05.538080Z","shell.execute_reply.started":"2025-12-25T17:15:05.531950Z","shell.execute_reply":"2025-12-25T17:15:05.537281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\n\ndef find_last_conv_layer(model):\n    last_conv = None\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Conv2d):\n            last_conv = module\n            last_conv_name = name\n    return last_conv, last_conv_name\n\n\ntarget_layer, layer_name = find_last_conv_layer(model)\n\nprint(\"Using Grad-CAM target layer:\", layer_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:26:04.500208Z","iopub.execute_input":"2025-12-25T17:26:04.500991Z","iopub.status.idle":"2025-12-25T17:26:04.506180Z","shell.execute_reply.started":"2025-12-25T17:26:04.500961Z","shell.execute_reply":"2025-12-25T17:26:04.505420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\n\ndef find_last_cnn_conv(model):\n    last_conv = None\n    last_name = None\n\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Conv2d):\n            # Skip patch embedding / projection layers\n            if \"proj\" in name.lower() or \"patch\" in name.lower():\n                continue\n            last_conv = module\n            last_name = name\n\n    return last_conv, last_name\n\n\ntarget_layer, layer_name = find_last_cnn_conv(model)\nprint(\"Using Grad-CAM CNN layer:\", layer_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:36:19.798670Z","iopub.execute_input":"2025-12-25T17:36:19.799011Z","iopub.status.idle":"2025-12-25T17:36:19.804576Z","shell.execute_reply.started":"2025-12-25T17:36:19.798983Z","shell.execute_reply":"2025-12-25T17:36:19.803973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gradcam = GradCAM(model, target_layer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:36:42.079606Z","iopub.execute_input":"2025-12-25T17:36:42.079930Z","iopub.status.idle":"2025-12-25T17:36:42.083707Z","shell.execute_reply.started":"2025-12-25T17:36:42.079904Z","shell.execute_reply":"2025-12-25T17:36:42.083011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(target_layer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:37:15.884557Z","iopub.execute_input":"2025-12-25T17:37:15.884892Z","iopub.status.idle":"2025-12-25T17:37:15.888877Z","shell.execute_reply.started":"2025-12-25T17:37:15.884866Z","shell.execute_reply":"2025-12-25T17:37:15.888318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get a batch of images\nimages, labels = next(iter(val_loader))\nimages = images.to(device)\n\n# Get predicted class for the first image\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(images)\n    class_idx = outputs[0].argmax().item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:38:55.598236Z","iopub.execute_input":"2025-12-25T17:38:55.598519Z","iopub.status.idle":"2025-12-25T17:38:55.758488Z","shell.execute_reply.started":"2025-12-25T17:38:55.598495Z","shell.execute_reply":"2025-12-25T17:38:55.757814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get a batch\nimages, labels = next(iter(val_loader))\nimages = images.to(device)\n\n# Prediction\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(images)\n    class_idx = outputs[0].argmax().item()\n\n# Original image (for overlay)\noriginal_img = images[0].cpu().permute(1,2,0).numpy()\noriginal_img = (original_img - original_img.min()) / (original_img.max() + 1e-8)\n\n# Generate Grad-CAM\ncam = gradcam.generate(images[0:1], class_idx)\n\n# Resize & overlay\ncam = cv2.resize(cam, (224, 224))\nheatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\noverlay = cv2.addWeighted(\n    np.uint8(255 * original_img), 0.6,\n    heatmap, 0.4, 0\n)\n\n# Save\nimport os\nos.makedirs(\"/kaggle/working/gradcam_outputs\", exist_ok=True)\nsave_path = \"/kaggle/working/gradcam_outputs/gradcam_sample.png\"\n\nplt.figure(figsize=(5,5))\nplt.imshow(overlay)\nplt.axis(\"off\")\nplt.tight_layout()\nplt.savefig(save_path, dpi=300)\nplt.close()\n\nsave_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:39:08.114928Z","iopub.execute_input":"2025-12-25T17:39:08.115566Z","iopub.status.idle":"2025-12-25T17:39:08.524562Z","shell.execute_reply.started":"2025-12-25T17:39:08.115540Z","shell.execute_reply":"2025-12-25T17:39:08.523995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\n\n# ===============================\n# 1. Get a validation batch\n# ===============================\nimages, labels = next(iter(val_loader))\nimages = images.to(device)\n\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(images)\n    class_idx = outputs[0].argmax().item()\n\n# ===============================\n# 2. Recover ORIGINAL image\n# ===============================\n# If your dataset normalizes images, undo it here if needed\noriginal_img = images[0].cpu().permute(1, 2, 0).numpy()\n\n# Normalize for visualization\noriginal_img = (original_img - original_img.min()) / (original_img.max() + 1e-8)\noriginal_img_uint8 = np.uint8(255 * original_img)\n\n# ===============================\n# 3. Generate Grad-CAM\n# ===============================\ncam = gradcam.generate(images[0:1], class_idx)\n\ncam = cv2.resize(cam, (224, 224))\ncam_uint8 = np.uint8(255 * cam)\n\nheatmap = cv2.applyColorMap(cam_uint8, cv2.COLORMAP_JET)\n\n# ===============================\n# 4. Overlay Grad-CAM on Image\n# ===============================\noverlay = cv2.addWeighted(\n    original_img_uint8, 0.6,\n    heatmap, 0.4,\n    0\n)\n\n# ===============================\n# 5. Save all images\n# ===============================\nos.makedirs(\"/kaggle/working/gradcam_outputs\", exist_ok=True)\n\ncv2.imwrite(\"/kaggle/working/gradcam_outputs/original.png\", original_img_uint8)\ncv2.imwrite(\"/kaggle/working/gradcam_outputs/gradcam_heatmap.png\", heatmap)\ncv2.imwrite(\"/kaggle/working/gradcam_outputs/gradcam_overlay.png\", overlay)\n\n# ===============================\n# 6. Display comparison (SIDE-BY-SIDE)\n# ===============================\nplt.figure(figsize=(12,4))\n\nplt.subplot(1,3,1)\nplt.imshow(original_img_uint8)\nplt.title(\"Original Chest X-ray\")\nplt.axis(\"off\")\n\nplt.subplot(1,3,2)\nplt.imshow(heatmap)\nplt.title(\"Grad-CAM Heatmap\")\nplt.axis(\"off\")\n\nplt.subplot(1,3,3)\nplt.imshow(overlay)\nplt.title(\"Grad-CAM Overlay\")\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.savefig(\n    \"/kaggle/working/gradcam_outputs/gradcam_comparison.png\",\n    dpi=300\n)\nplt.show()\n\n\"/kaggle/working/gradcam_outputs/gradcam_comparison.png\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:41:46.635866Z","iopub.execute_input":"2025-12-25T17:41:46.636489Z","iopub.status.idle":"2025-12-25T17:41:47.480030Z","shell.execute_reply.started":"2025-12-25T17:41:46.636461Z","shell.execute_reply":"2025-12-25T17:41:47.479210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\n\n# =========================================================\n# ASSUMPTIONS\n# - model is loaded and on device\n# - val_loader exists\n# - gradcam object already created with correct target layer\n# =========================================================\n\n# -----------------------------\n# 1. Get a validation sample\n# -----------------------------\nimages, labels = next(iter(val_loader))\nimages = images.to(device)\n\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(images)\n    class_idx = outputs[0].argmax().item()\n\n# -----------------------------\n# 2. Recover ORIGINAL chest X-ray\n#    (handles normalized tensors safely)\n# -----------------------------\norig = images[0].cpu().permute(1, 2, 0).numpy()\n\n# If image is single-channel, convert to 3-channel\nif orig.shape[-1] == 1:\n    orig = np.repeat(orig, 3, axis=-1)\n\n# Normalize ONLY for visualization\norig = (orig - orig.min()) / (orig.max() + 1e-8)\norig_uint8 = np.uint8(255 * orig)\n\n# -----------------------------\n# 3. Generate Grad-CAM\n# -----------------------------\ncam = gradcam.generate(images[0:1], class_idx)\n\n# Smooth upsampling (IMPORTANT)\ncam = cv2.resize(cam, (224, 224), interpolation=cv2.INTER_CUBIC)\ncam = np.clip(cam, 0, 1)\ncam_uint8 = np.uint8(255 * cam)\n\n# -----------------------------\n# 4. Heatmap & Overlay\n# -----------------------------\nheatmap = cv2.applyColorMap(cam_uint8, cv2.COLORMAP_JET)\n\n# Balanced overlay (clinically readable)\noverlay = cv2.addWeighted(\n    orig_uint8, 0.75,\n    heatmap, 0.25,\n    0\n)\n\n# -----------------------------\n# 5. Save outputs\n# -----------------------------\nos.makedirs(\"/kaggle/working/gradcam_outputs\", exist_ok=True)\n\ncv2.imwrite(\"/kaggle/working/gradcam_outputs/original.png\", orig_uint8)\ncv2.imwrite(\"/kaggle/working/gradcam_outputs/gradcam_heatmap.png\", heatmap)\ncv2.imwrite(\"/kaggle/working/gradcam_outputs/gradcam_overlay.png\", overlay)\n\n# -----------------------------\n# 6. Side-by-side comparison\n# -----------------------------\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(orig_uint8)\nplt.title(\"Original Chest X-ray\")\nplt.axis(\"off\")\n\nplt.subplot(1, 3, 2)\nplt.imshow(heatmap)\nplt.title(\"Grad-CAM Heatmap\")\nplt.axis(\"off\")\n\nplt.subplot(1, 3, 3)\nplt.imshow(overlay)\nplt.title(\"Grad-CAM Overlay\")\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.savefig(\n    \"/kaggle/working/gradcam_outputs/gradcam_comparison.png\",\n    dpi=300,\n    bbox_inches=\"tight\"\n)\nplt.show()\n\n\"/kaggle/working/gradcam_outputs/gradcam_comparison.png\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:43:47.781536Z","iopub.execute_input":"2025-12-25T17:43:47.782069Z","iopub.status.idle":"2025-12-25T17:43:48.627629Z","shell.execute_reply.started":"2025-12-25T17:43:47.782040Z","shell.execute_reply":"2025-12-25T17:43:48.626918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    roc_auc_score\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:49:38.216284Z","iopub.execute_input":"2025-12-25T17:49:38.216832Z","iopub.status.idle":"2025-12-25T17:49:38.220438Z","shell.execute_reply.started":"2025-12-25T17:49:38.216799Z","shell.execute_reply":"2025-12-25T17:49:38.219754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model_full_metrics(model, dataloader, device):\n    model.eval()\n\n    all_preds = []\n    all_targets = []\n\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images)\n            probs = torch.sigmoid(outputs)\n\n            all_preds.append(probs.cpu().numpy())\n            all_targets.append(labels.cpu().numpy())\n\n    y_pred = np.vstack(all_preds)\n    y_true = np.vstack(all_targets)\n\n    # Binary predictions (threshold = 0.5)\n    y_bin = (y_pred >= 0.5).astype(int)\n\n    # Metrics\n    accuracy = accuracy_score(y_true, y_bin)\n    precision = precision_score(y_true, y_bin, average=\"macro\", zero_division=0)\n    recall = recall_score(y_true, y_bin, average=\"macro\", zero_division=0)\n    f1 = f1_score(y_true, y_bin, average=\"macro\", zero_division=0)\n\n    auc = roc_auc_score(\n        y_true,\n        y_pred,\n        average=\"macro\",\n        multi_class=\"ovr\"\n    )\n\n    return {\n        \"Accuracy\": round(accuracy * 100, 2),\n        \"Precision\": round(precision * 100, 2),\n        \"Recall\": round(recall * 100, 2),\n        \"F1-score\": round(f1 * 100, 2),\n        \"AUC\": round(auc * 100, 2)\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:49:46.896614Z","iopub.execute_input":"2025-12-25T17:49:46.897162Z","iopub.status.idle":"2025-12-25T17:49:46.904839Z","shell.execute_reply.started":"2025-12-25T17:49:46.897136Z","shell.execute_reply":"2025-12-25T17:49:46.904079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"table1_results = []\n\nmodels = {\n    \"CNN Baseline\": (\"cnn_baseline.pt\", CNNBaseline),\n    \"CNN + Transformer (No Optimization)\": (\"cnn_transformer_no_opt.pt\", HybridCNNTransformer),\n    \"Proposed UO-HCT (GWO)\": (\"hybrid_uo_hct.pt\", HybridCNNTransformer)\n}\n\nfor name, (ckpt, model_cls) in models.items():\n    model = model_cls(num_classes=5).to(device)\n    model.load_state_dict(\n        torch.load(f\"/kaggle/working/{ckpt}\", map_location=device)\n    )\n\n    metrics = evaluate_model_full_metrics(model, test_loader, device)\n\n    metrics[\"Model\"] = name\n    table1_results.append(metrics)\n\ntable1_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:49:52.910247Z","iopub.execute_input":"2025-12-25T17:49:52.910727Z","iopub.status.idle":"2025-12-25T17:49:52.918789Z","shell.execute_reply.started":"2025-12-25T17:49:52.910701Z","shell.execute_reply":"2025-12-25T17:49:52.917826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get install -y graphviz\n!pip install graphviz\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T18:08:27.731580Z","iopub.execute_input":"2025-12-25T18:08:27.732246Z","iopub.status.idle":"2025-12-25T18:08:34.466492Z","shell.execute_reply.started":"2025-12-25T18:08:27.732220Z","shell.execute_reply":"2025-12-25T18:08:34.465737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from graphviz import Digraph\n\ndot = Digraph(\n    name=\"UO-HCT_Architecture\",\n    format=\"pdf\",\n    graph_attr={\"rankdir\": \"TB\", \"fontsize\": \"10\"}\n)\n\n# ===== Nodes =====\ndot.node(\"Input\", \"Input Chest X-ray\\n(224 Ã— 224)\")\ndot.node(\"CNN\", \"CNN Backbone\\n(ResNet18)\")\ndot.node(\"Feat\", \"Convolutional\\nFeature Maps\")\ndot.node(\"Token\", \"Feature Tokenization\\n+ Positional Encoding\")\ndot.node(\"Trans\", \"Transformer Encoder\\n(Multi-Head Attention)\")\ndot.node(\"FC\", \"Fully Connected\\nClassification Head\")\ndot.node(\"Out\", \"Multi-Disease\\nProbabilities\")\n\ndot.node(\"PSO\", \"PSO / GWO\\nHyperparameter Optimization\", shape=\"ellipse\")\ndot.node(\"CAM\", \"Grad-CAM\\nExplainability\", shape=\"ellipse\")\n\n# ===== Edges =====\ndot.edge(\"Input\", \"CNN\")\ndot.edge(\"CNN\", \"Feat\")\ndot.edge(\"Feat\", \"Token\")\ndot.edge(\"Token\", \"Trans\")\ndot.edge(\"Trans\", \"FC\")\ndot.edge(\"FC\", \"Out\")\n\n# Side connections\ndot.edge(\"PSO\", \"CNN\", style=\"dashed\")\ndot.edge(\"PSO\", \"Trans\", style=\"dashed\")\ndot.edge(\"Feat\", \"CAM\", style=\"dashed\")\n\n# ===== SAVE FILE =====\ndot.render(\"/kaggle/working/architecture2\", cleanup=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T18:14:14.167750Z","iopub.execute_input":"2025-12-25T18:14:14.168535Z","iopub.status.idle":"2025-12-25T18:14:14.201870Z","shell.execute_reply.started":"2025-12-25T18:14:14.168505Z","shell.execute_reply":"2025-12-25T18:14:14.201145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8, 10))\nax.axis(\"off\")\n\nboxes = {\n    \"Input\": (0.5, 0.9, \"Input Chest X-ray\\n(224Ã—224)\"),\n    \"CNN\": (0.5, 0.8, \"CNN Backbone\\n(ResNet18)\"),\n    \"Feat\": (0.5, 0.7, \"Conv Feature Maps\"),\n    \"Token\": (0.5, 0.6, \"Tokenization\\n+ Positional Encoding\"),\n    \"Trans\": (0.5, 0.5, \"Transformer Encoder\\n(MHSA)\"),\n    \"FC\": (0.5, 0.4, \"Fully Connected\\nLayer\"),\n    \"Out\": (0.5, 0.3, \"Multi-Disease\\nProbabilities\"),\n}\n\n# Draw boxes\nfor x, y, text in boxes.values():\n    ax.text(\n        x, y, text,\n        ha=\"center\", va=\"center\",\n        bbox=dict(boxstyle=\"round\", fc=\"white\")\n    )\n\n# Draw arrows\nkeys = list(boxes.keys())\nfor i in range(len(keys)-1):\n    ax.annotate(\n        \"\", \n        xy=(0.5, boxes[keys[i+1]][1] + 0.03),\n        xytext=(0.5, boxes[keys[i]][1] - 0.03),\n        arrowprops=dict(arrowstyle=\"->\")\n    )\n\nplt.savefig(\"architecture.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T18:11:49.290052Z","iopub.execute_input":"2025-12-25T18:11:49.290698Z","iopub.status.idle":"2025-12-25T18:11:49.916902Z","shell.execute_reply.started":"2025-12-25T18:11:49.290672Z","shell.execute_reply":"2025-12-25T18:11:49.916210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}